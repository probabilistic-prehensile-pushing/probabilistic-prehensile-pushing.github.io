<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Our method generates trajectories for prehensile pushing.">
  <meta name="keywords" content="prehensile pushing, trajectory optimization">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Pushing Everything Everywhere All At Once: Probabilistic Prehensile Pushing</title>

  <meta property="og:image" content="resources/og_image.jpg"/>
	<meta property="og:title" content="Pushing Everything Everywhere All At Once: Probabilistic Prehensile Pushing" />
	<meta property="og:description" content="Our method formulates prehensile pushing as a continuous non-linear trajectory optimization problem" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VFNFH9CKNX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-VFNFH9CKNX');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./resources/carousel-horse.png"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
  <!-- <script src="./static/js/index.js"></script> -->
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <!-- <video id="banner" autoplay muted loop playsinline height="100%">
          <source src="resources/carousel.mp4"
                  type="video/mp4">
        </video> -->
      </div>
      <br>

      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title" style="font-size: 2.2rem;">Pushing Everything Everywhere All At Once: Probabilistic Prehensile Pushing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Patrizio Perugini</a>,
            </span>
            <span class="author-block">
              <a href="">Jens Lundell</a>,
            </span>
            <span class="author-block">
              <a href="">Katharina Friedl</a>,
            </span>
            <span class="author-block">
              <a href="">Danica Kragic</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Division of Robotics, Perception and Learning (RPL), KTH</span>
          </div>

          <!-- <div class="is-size-5 publication-authors"> -->
            <!-- <span class="author-block">IEEE Robotics and Automation Letters (RA-L)</span> -->
          <!-- </div> -->

          <div class="column has-text-centered">
            <!-- <div class="publication-links"> -->
              <!-- PDF Link. -->
              <!-- <span class="link-block"> -->
                <!-- <a href="https://arxiv.org/abs/2402.02989" -->
                   <!-- class="external-link button is-normal is-rounded is-dark"> -->
                  <!-- <span class="icon"> -->
                      <!-- <i class="fas fa-file-pdf"></i> -->
                  <!-- </span> -->
                  <!-- <span>Paper</span> -->
                <!-- </a> -->
              <!-- </span> -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/MAJyj3V0_kI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span> -->
                <!-- </a> -->
              <!-- </span> -->
              <!-- <span class="link-block"> -->
                <!-- <a href="https://github.com/jsll/simulating_prehensile_pushing/tree/refactoring_velocity" -->
                   <!-- class="external-link button is-normal is-rounded is-dark"> -->
                  <!-- <span class="icon"> -->
                      <!-- <i class="fab fa-github"></i> -->
                  <!-- </span> -->
                  <!-- <span>Code</span> -->
                  <!-- </a> -->
              <!-- </span> -->
            <!-- </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser"> -->
  <!-- <div class="container is-max-desktop"> -->
    <!-- <div class="hero-body"> -->
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%"> -->
        <!-- <source src="resources/dexdiff_compress_part0.mp4" -->
                <!-- type="video/mp4"> -->
      <!-- </video> -->
      <!-- <h2 class="subtitle"> -->
        <!-- Our method can generate high-quality grasp poses for unknown objects based on 3D partial point clouds. We conducted our study in both simulation and real world. -->
      <!-- </h2> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->


<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We address prehensile pushing, the problem of manipulating a grasped object by pushing against the environment. Our solution is an efficient nonlinear trajectory optimization problem relaxed from an exact mixed integer non-linear trajectory optimization formulation. The critical insight is recasting the external pushers (environment) as a discrete probability distribution instead of binary variables and minimizing the entropy of the distribution. The probabilistic reformulation allows all pushers to be used simultaneously, but at the optimum, the probability mass concentrates onto one due to the entropy minimization. We numerically compare our method against a state-of-the-art sampling-based baseline on a prehensile pushing task. The results demonstrate that our method finds trajectories 8 times faster and at a 20 times lower cost than the baseline. Finally, we demonstrate that a \textcolor{blue}{simulated and real} \panda{} robot can successfully manipulate different objects following the trajectories proposed by our method.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MAJyj3V0_kI" title="CAPGrasp" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->

  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
      <h2 class="title is-3">Trajectory optimization example</h2>
      <div class="publication-video">
        <div class="content has-text-centered">
          <video id="trajectory-optimizatoin" autoplay controls muted preload loop playsinline>
            <source src="resources/numerical_evaluation.mp4"
                    type="video/mp4">
          </video>
      </div>
    </div>
  </div>
 
</div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
      <h2 class="title is-3">Simulation experiments</h2>
      <div class="publication-video">
        <div class="content has-text-centered">
          <video id="table-picking" autoplay controls muted preload loop playsinline>
            <source src="resources/dexdiff_compress_part2.mp4"
                    type="video/mp4">
          </video>
      </div>
    </div>
  </div>

</div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
      <h2 class="title is-3">Real robotic experiments</h2>
      <div class="publication-video">
        <div class="content has-text-centered">
          <video id="table-picking" autoplay controls muted preload loop playsinline>
            <source src="resources/real_robot_experiment.mp4"
                    type="video/mp4">
          </video>
      </div>
    </div>
  </div>

</div>

</div>

</div>

</section>


<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Single-Image 3D Reconstruction</h2>

        <h2 class="title is-5">Real Horse Images and Realistic Paintings</h2>
        <div class="content has-text-justified">
          <p>
            After training, given a single image of a new instance, the model reconstructs articulated 3D shape and appearance of it, which can be animated and re-rendered from arbitrary viewpoints.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline>
            <source src="resources/RAL-web-video-compress.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

        <h2 class="title is-5">Generalisation to Abstract Horse Drawings and Artefacts</h2>
        <div class="content has-text-justified">
          <p>
            The model also generalises to abstract drawings and artefacts, despite being trained only on real images.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline>
            <source src="resources/horse_abstract.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

        <h2 class="title is-5">Other Animal Categories: Giraffes, Zebras and Cows</h2>
        <div class="content has-text-justified">
          <p>
            After finetuning, our model also generalises to various animal categories with highly different underlying shapes.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline>
            <source src="resources/other_animals.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

        <h2 class="title is-5">Frame-by-Frame Reconstruction on Videos</h2>
        <div class="content has-text-justified">
          <p>
            We run the model on videos frame by frame, and obtain temporally consistent reconstructions.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline style="max-width: 86%; display: block; margin: auto;">
            <source src="resources/video_recon.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

      </div>
    </div>

  </div>
</section> -->


<!-- <section class="section" id="BibTeX"> -->
  <!-- <div class="container is-max-desktop content"> -->
    <!-- <h2 class="title">BibTeX</h2> -->
    <!-- <pre><code>@ARTICLE{10753039, -->
      <!-- author={Weng, Zehang and Lu, Haofei and Kragic, Danica and Lundell, Jens}, -->
      <!-- journal={IEEE Robotics and Automation Letters},  -->
      <!-- title={DexDiffuser: Generating Dexterous Grasps With Diffusion Models},  -->
      <!-- year={2024}, -->
      <!-- volume={9}, -->
      <!-- number={12}, -->
      <!-- pages={11834-11840}, -->
      <!-- doi={10.1109/LRA.2024.3498776}} -->
<!-- </code></pre> -->
  <!-- </div> -->
<!-- </section> -->

<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    <p>
      This work was supported by the Swedish Research Council, the Knut and Alice Wallenberg Foundation, the European Research Council (ERC-BIRD-884807).
    </p>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This webpage template is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
